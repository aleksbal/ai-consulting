<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
  <meta charset="UTF-8" />
  <title>RAG Basics</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="RAG Basics: what Retrieval-Augmented Generation is, how it works, where it helps, and where it doesn‚Äôt ‚Äî explained in simple business language." />

  <link rel="icon" type="image/svg+xml" href="../favicon.svg" />

  <!-- Tailwind + Typography plugin -->
  <script src="https://cdn.tailwindcss.com?plugins=typography"></script>
  <script>
    tailwind.config = { darkMode: 'class' };
  </script>

  <style>
    .lang-toggle-active {
      background-color: rgba(45,212,191,0.15);
      color: rgb(45,212,191);
    }
  </style>

  <link rel="stylesheet" href="../css/prose.css">
</head>

<body class="bg-slate-50 text-slate-900 dark:bg-slate-950 dark:text-slate-100 antialiased">

<script>
  let currentLang = "en";
  let currentTheme = "light";

  function setTheme(theme, store = true) {
    currentTheme = theme;
    document.documentElement.classList.toggle("dark", theme === "dark");
    if (store) {
      try { localStorage.setItem("theme", theme); } catch (e) {}
    }
    const btn = document.getElementById("btn-theme");
    if (btn) btn.textContent = theme === "dark" ? "‚òÄÔ∏è" : "üåô";
  }

  function initTheme() {
    let t = null;
    try { t = localStorage.getItem("theme"); } catch (e) {}
    if (t === "light" || t === "dark") {
      setTheme(t, false);
    } else {
      setTheme(
        window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light",
        false
      );
    }
  }

  function toggleTheme() {
    setTheme(currentTheme === "dark" ? "light" : "dark");
  }

  function setLang(lang) {
    currentLang = lang;
    try { localStorage.setItem("lang", lang); } catch (e) {}
    document.documentElement.lang = lang;
    const enBtn = document.getElementById("btn-lang-en");
    const deBtn = document.getElementById("btn-lang-de");
    if (enBtn && deBtn) {
      enBtn.classList.toggle("lang-toggle-active", lang === "en");
      deBtn.classList.toggle("lang-toggle-active", lang === "de");
    }
  }

  function initLang() {
    let stored = null;
    try { stored = localStorage.getItem("lang"); } catch (e) {}
    if (stored === "en" || stored === "de") {
      setLang(stored);
    } else {
      const navLang = (navigator.language || "").toLowerCase().startsWith("de") ? "de" : "en";
      setLang(navLang);
    }
  }

  document.addEventListener("DOMContentLoaded", () => {
    initTheme();
    initLang();
    const yearEl = document.getElementById("year");
    if (yearEl) yearEl.textContent = new Date().getFullYear();
  });
</script>

<div class="min-h-screen flex flex-col">

  <!-- HEADER -->
  <header class="sticky top-0 z-40 border-b border-slate-200 dark:border-slate-800/70 bg-white/80 dark:bg-slate-950/80 backdrop-blur">
    <div class="max-w-6xl mx-auto px-4 py-4 flex items-center justify-between gap-4">
      <div class="flex items-center gap-2">
        <div class="h-8 w-8 rounded-xl bg-gradient-to-br from-emerald-400 to-cyan-500 flex items-center justify-center text-sm font-bold text-slate-950">
          M
        </div>
        <div>
          <div class="font-semibold tracking-tight">Modest AI Studio</div>
          <div class="text-xs text-slate-500 dark:text-slate-400">
             AI consulting for real-world projects
          </div>
        </div>
      </div>

      <nav class="hidden md:flex gap-6 text-sm text-slate-600 dark:text-slate-300">
        <a href="../index.html#services" class="hover:text-emerald-600 dark:hover:text-emerald-300">Services</a>
        <a href="../index.html#process" class="hover:text-emerald-600 dark:hover:text-emerald-300">How it works</a>
        <a href="../index.html#use-cases" class="hover:text-emerald-600 dark:hover:text-emerald-300">Use cases</a>
        <a href="../articles.html" class="hover:text-emerald-600 dark:hover:text-emerald-300">Articles</a>
        <a href="../index.html#about" class="hover:text-emerald-600 dark:hover:text-emerald-300">About</a>
        <a href="../index.html#contact" class="hover:text-emerald-600 dark:hover:text-emerald-300">Contact</a>
      </nav>

      <div class="flex items-center gap-3">
        <div class="flex rounded-full border border-slate-300 dark:border-slate-700 text-xs overflow-hidden">
          <button id="btn-lang-en" onclick="setLang('en')" class="px-2 py-1 hover:bg-slate-100 dark:hover:bg-slate-800">EN</button>
          <button id="btn-lang-de" onclick="setLang('de')" class="px-2 py-1 hover:bg-slate-100 dark:hover:bg-slate-800">DE</button>
        </div>
        <button id="btn-theme" onclick="toggleTheme()" class="text-sm rounded-full border px-2 py-1 border-slate-300 dark:border-slate-700 hover:bg-slate-100 dark:hover:bg-slate-800"></button>
        <a href="../index.html#contact" class="hidden md:inline-flex items-center rounded-full border border-emerald-500/70 px-4 py-1.5 text-sm font-medium text-emerald-800 dark:text-emerald-100 hover:bg-emerald-500/10">
          Free intro call
        </a>
      </div>
    </div>
  </header>

  <!-- MAIN -->
  <main class="flex-1">
    <article class="bg-slate-50 dark:bg-slate-950">
      <div class="max-w-3xl mx-auto px-4 py-10 lg:py-14">

        <p class="text-xs uppercase tracking-[0.25em] text-emerald-600/80 dark:text-emerald-300/80 mb-3">
          RAG ¬∑ Knowledge ¬∑ Search
        </p>

        <h1 class="text-3xl font-semibold tracking-tight mb-3">
          Testing and evaluation of Retrieval-Augmented Generation systems
        </h1>

        <p class="text-xs text-slate-500 dark:text-slate-400 mb-6">
          5‚Äì7 min read ¬∑ For IT &amp; business stakeholders
        </p>

        <!-- Body -->
        <div class="prose prose-slate dark:prose-invert max-w-none">
          <p>
            Retrieval-Augmented Generation (RAG) systems combine information retrieval with large
            language model inference. As a result, their evaluation differs from traditional NLP
            systems and requires assessing both the retrieval stage and the generation stage.
            In 2025, most production-grade RAG systems are tested using automated evaluation
            frameworks that rely on LLM-based scoring rather than exclusively on human judgment.
          </p>

          <h2>General evaluation approach</h2>

          <p>
            Modern RAG evaluation typically separates the system into two conceptual components:
            retrieval and generation. Each component is evaluated independently, allowing teams
            to identify whether failures originate from missing or irrelevant context, or from
            incorrect use of retrieved information during answer generation.
          </p>

          <p>
            Due to the cost and limited scalability of fully human-labeled datasets, many
            evaluation pipelines employ an <em>LLM-as-a-judge</em> approach. In this setup,
            a language model is used to score the quality of retrieved context and generated
            answers according to predefined criteria.
          </p>

          <h2>Evaluation frameworks</h2>

          <p>
            Several open-source and commercial frameworks are commonly used to evaluate RAG
            systems. These tools provide standardized metrics and abstractions for assessing
            retrieval quality, answer relevance, and factual grounding.
          </p>

          <ul>
            <li>
              <strong>Ragas (RAG Assessment)</strong> focuses on metric-based evaluation of
              retrieval and generation, often without requiring manually curated ground-truth
              answers for every test case.
            </li>
            <li>
              <strong>TruLens</strong> applies a structured evaluation model commonly referred
              to as the ‚ÄúRAG Triad‚Äù, which evaluates context relevance, groundedness, and answer
              relevance.
            </li>
            <li>
              <strong>DeepEval</strong> emphasizes automated testing workflows and CI/CD
              integration, treating LLM evaluations similarly to software unit tests with
              pass/fail criteria.
            </li>
            <li>
              <strong>Maxim AI</strong> provides an end-to-end platform for offline evaluation,
              simulation, and production monitoring of RAG applications.
            </li>
          </ul>

          <h2>Core evaluation metrics</h2>

          <p>
            While individual frameworks differ in terminology, most RAG evaluations rely on
            a shared set of core metrics. These metrics are commonly grouped into retrieval
            metrics and generation metrics.
          </p>

          <h3>Retrieval metrics</h3>

          <ul>
            <li>
              <strong>Context precision</strong> measures whether the retrieved document chunks
              are relevant to the input query.
            </li>
            <li>
              <strong>Context recall</strong> measures whether all information required to
              answer the query is present in the retrieved context.
            </li>
          </ul>

          <h3>Generation metrics</h3>

          <ul>
            <li>
              <strong>Faithfulness (groundedness)</strong> measures whether the generated answer
              is supported by the retrieved context and does not introduce unsupported claims.
            </li>
            <li>
              <strong>Answer relevance</strong> measures how well the generated response addresses
              the intent of the original query.
            </li>
          </ul>

          <h2>Evaluation datasets</h2>

          <p>
            RAG systems are typically evaluated using a curated evaluation dataset, often
            referred to as a ‚Äúgolden dataset‚Äù. Such datasets usually consist of realistic
            questions, reference answers or expected facts, and the corresponding source
            documents from which answers should be derived.
          </p>

          <p>
            Because manual dataset creation is time-consuming, many evaluation workflows
            augment curated datasets with synthetically generated test cases. In this approach,
            language models generate questions and reference answers directly from source
            documents, increasing coverage across topics and document structures.
          </p>

          <h2>Use of public benchmarks</h2>

          <p>
            Public benchmarks are commonly used to compare RAG architectures or to study
            specific failure modes. These datasets are primarily intended for research and
            benchmarking purposes and are not a substitute for domain-specific evaluation data.
          </p>

          <ul>
            <li>
              <strong>FRAMES</strong> evaluates multi-document and multi-hop reasoning in
              retrieval-augmented systems.
            </li>
            <li>
              <strong>RAGTruth</strong> focuses on detecting hallucinations and unsupported
              statements in RAG outputs.
            </li>
            <li>
              <strong>FEVER</strong> is a fact verification dataset used to assess evidence-based
              claim validation against structured sources.
            </li>
          </ul>

          <h2>Limitations of automated evaluation</h2>

          <p>
            Although LLM-based evaluation enables scalable and repeatable testing, it is not
            fully deterministic and may vary depending on prompt formulation and model choice.
            For critical use cases, automated evaluation is typically complemented by periodic
            human review and regression testing based on real production failures.
          </p>

          <h2>Summary</h2>

          <p>
            Testing RAG systems requires evaluating both retrieval quality and answer generation.
            In practice, most teams rely on automated evaluation frameworks, curated and synthetic
            datasets, and a limited set of core metrics to detect regressions and guide system
            improvements. This approach enables continuous evaluation without excessive manual
            effort while maintaining acceptable reliability in production systems.
          </p>


        </div>
      </div>
    </article>
  </main>

  <!-- FOOTER -->
  <footer class="border-t border-slate-200 dark:border-slate-900 bg-white dark:bg-slate-950">
    <div class="max-w-6xl mx-auto px-4 py-4 flex flex-col sm:flex-row items-center justify-between gap-2 text-xs text-slate-500">
      <div>¬© <span id="year"></span> Modest AI Studio. Alle Rechte vorbehalten.</div>
      <div class="flex gap-4">
        <a href="../impressum.html" class="hover:text-emerald-500">Impressum</a>
        <a href="../datenschutz.html" class="hover:text-emerald-500">Datenschutzerkl√§rung</a>
      </div>
    </div>
  </footer>
</div>

</body>
</html>
